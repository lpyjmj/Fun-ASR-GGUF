{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ead4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\funasr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import onnxruntime\n",
    "import torch\n",
    "import torchaudio\n",
    "from pydub import AudioSegment\n",
    "from transformers import AutoTokenizer\n",
    "from llama_cpp import (\n",
    "    Llama,\n",
    "    llama_batch_init,\n",
    "    llama_batch_free,\n",
    "    llama_decode,\n",
    "    llama_get_logits,\n",
    "    llama_kv_self_clear,  # 新版 API：清理缓存\n",
    ")\n",
    "\n",
    "# =========================================================================\n",
    "# 配置部分\n",
    "# =========================================================================\n",
    "\n",
    "# 日志设置\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"inference_refine.log\", encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 模型路径\n",
    "model_dir = r'./model-gguf'\n",
    "tokenizer_path = f'{model_dir}/Qwen3-0.6B'\n",
    "\n",
    "# ONNX 模型\n",
    "onnx_encoder = f'{model_dir}/FunASR_Nano_Encoder.onnx'       # Audio Encoder\n",
    "onnx_embed = f'{model_dir}/FunASR_Nano_Decoder_Embed.onnx'  # Text/Prompt Embedder\n",
    "\n",
    "# GGUF 模型 (用于解码)\n",
    "gguf_model_path = f'{model_dir}/qwen3-0.6b-asr.gguf'\n",
    "\n",
    "# 音频处理参数 (需与模型训练时一致)\n",
    "SAMPLE_RATE = 16000\n",
    "USE_NORMALIZER = True\n",
    "MAX_INPUT_AUDIO_LENGTH = 320000 \n",
    "SLIDING_WINDOW = 0 # 0 表示根据音频长度自动分段 (这里简化为一次性处理或整段)\n",
    "\n",
    "# 模型参数\n",
    "MAX_SEQ_LEN = 1024\n",
    "STOP_TOKEN = [151643, 151645] # Qwen 的特殊停止 Token\n",
    "MAX_THREADS = 0 # 0 = Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e772122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 辅助函数\n",
    "# =========================================================================\n",
    "\n",
    "def normalizer(_audio, target_value=8192.0):\n",
    "    \"\"\"音频归一化处理\"\"\"\n",
    "    _audio = _audio.astype(np.float32)\n",
    "    rms = np.sqrt(np.mean((_audio * _audio), dtype=np.float32), dtype=np.float32)\n",
    "    _audio *= (target_value / (rms + 1e-7))\n",
    "    np.clip(_audio, -32768.0, 32767.0, out=_audio)\n",
    "    return _audio.astype(np.int16)\n",
    "\n",
    "def decode_with_pure_embeddings(llm_obj, audio_embeddings, max_new_tokens=200):\n",
    "    \"\"\"\n",
    "    纯 Embedding 解码函数 (Pure Embedding Decoding)\n",
    "    \n",
    "    原理：\n",
    "    FunASR 的 Encoder 输出的 Embedding 已经融合了音频特征和任务 Prompt 的语义。\n",
    "    因此，我们直接将这个 \"Fused Embedding\" 注入到 LLM 中，不需要额外拼接 Text Prefix (如 <|im_start|>)。\n",
    "    这避免了 \"Double Prompt\" 导致的分布不匹配问题。\n",
    "    \n",
    "    参数:\n",
    "    llm_obj: Llama 对象\n",
    "    audio_embeddings: Numpy 数组 (Shape: [seq_len, 1024])\n",
    "    max_new_tokens: 最大生成长度\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 准备数据\n",
    "    embeds = audio_embeddings.squeeze()\n",
    "    if len(embeds.shape) == 1:\n",
    "        embeds = embeds.reshape(1, -1)\n",
    "    \n",
    "    n_audio_tokens, n_dim = embeds.shape\n",
    "    logger.info(f\"注入 Audio Embeddings Shape: {embeds.shape}\")\n",
    "\n",
    "    # 2. 初始化 Batch\n",
    "    # batch_embd: 用于存放 Audio Embeddings (embd=n_dim, token=NULL)\n",
    "    batch_embd = llama_batch_init(n_audio_tokens, n_dim, 1)        \n",
    "    \n",
    "    # batch_text: 用于存放生成的 Token IDs (embd=0, token=分配内存)\n",
    "    batch_text = llama_batch_init(1, 0, 1)\n",
    "\n",
    "    ctx = llm_obj.ctx\n",
    "    \n",
    "    # 3. 清理上下文缓存 (KV Cache)\n",
    "    llama_kv_self_clear(llm_obj.ctx) \n",
    "    \n",
    "    try:\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 阶段 A: 注入融合 Embedding (Audio + Prompt fused)\n",
    "        # ---------------------------------------------------------------------\n",
    "        logger.info(\"正在注入融合 Embedding...\")\n",
    "        \n",
    "        batch_embd.n_tokens = n_audio_tokens\n",
    "        llm_obj.n_tokens = 0 # 重置 LLM 内部计数器\n",
    "        \n",
    "        for i in range(n_audio_tokens):\n",
    "            batch_embd.pos[i] = i\n",
    "            batch_embd.n_seq_id[i] = 1\n",
    "            batch_embd.seq_id[i][0] = 0\n",
    "            \n",
    "            # 只在最后一个 Token 开启 Logits 计算，用于预测第一个生成的文本 Token\n",
    "            batch_embd.logits[i] = 1 if i == n_audio_tokens - 1 else 0\n",
    "\n",
    "        # 使用 ctypes.memmove 高效拷贝 Numpy 数据到 C 指针\n",
    "        if not embeds.flags['C_CONTIGUOUS']:\n",
    "            embeds = np.ascontiguousarray(embeds)\n",
    "        \n",
    "        ctypes.memmove(batch_embd.embd, embeds.ctypes.data, embeds.nbytes)\n",
    "        \n",
    "        # 执行解码\n",
    "        if llama_decode(ctx, batch_embd) != 0:\n",
    "             raise RuntimeError(\"Audio embedding decoding failed\")\n",
    "        \n",
    "        llm_obj.n_tokens += n_audio_tokens\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 阶段 B: 文本生成 (Greedy Search)\n",
    "        # ---------------------------------------------------------------------\n",
    "        generated_text = \"\"\n",
    "        logger.info(f\"开始生成文本 (最大 {max_new_tokens} tokens)...\\n\")\n",
    "        \n",
    "        eos_token = llm_obj.token_eos()\n",
    "        vocab_size = llm_obj.n_vocab()\n",
    "        \n",
    "        batch_text.n_tokens = 1\n",
    "        \n",
    "        gen_start_time = time.time()\n",
    "        tokens_generated = 0\n",
    "        \n",
    "        for step in range(max_new_tokens):\n",
    "            # 1. 获取 Logits\n",
    "            logits_ptr = llama_get_logits(ctx)\n",
    "            logits_arr = np.ctypeslib.as_array(logits_ptr, shape=(vocab_size,))\n",
    "            \n",
    "            # 2. 贪婪采样 (Argmax)\n",
    "            token_id = int(np.argmax(logits_arr))\n",
    "            \n",
    "            # 3. 检查停止条件\n",
    "            if token_id == eos_token or token_id in STOP_TOKEN:\n",
    "                break\n",
    "                \n",
    "            # 4. 解码 token 为文本\n",
    "            try:\n",
    "                text_piece = llm_obj.detokenize([token_id]).decode('utf-8', errors='ignore')\n",
    "                print(text_piece, end=\"\", flush=True)\n",
    "                generated_text += text_piece\n",
    "                tokens_generated += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "                \n",
    "            # 5. 把生成的 Token 喂回去 (Autoregressive)\n",
    "            batch_text.token[0] = token_id\n",
    "            batch_text.pos[0] = llm_obj.n_tokens\n",
    "            # 必须显式初始化 seq_id，否则会导致随机内存访问错误 (access violation)\n",
    "            batch_text.n_seq_id[0] = 1\n",
    "            batch_text.seq_id[0][0] = 0\n",
    "            batch_text.logits[0] = 1\n",
    "            \n",
    "            if llama_decode(ctx, batch_text) != 0:\n",
    "                break\n",
    "            \n",
    "            llm_obj.n_tokens += 1\n",
    "            \n",
    "        print('\\n\\n')\n",
    "        gen_duration = time.time() - gen_start_time\n",
    "        tps = tokens_generated / gen_duration if gen_duration > 0 else 0\n",
    "\n",
    "        logger.info(f\"解码速度: {tps:.2f} tokens/s ({tokens_generated} tokens in {gen_duration:.2f}s)\\n\\n\")\n",
    "        \n",
    "    finally:\n",
    "        # 释放资源\n",
    "        llama_batch_free(batch_embd)\n",
    "        llama_batch_free(batch_text)\n",
    "\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6d4af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading ONNX models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './model-gguf/Qwen3-0.6B' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading GGUF model: ./model-gguf/qwen3-0.6b-asr.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGUF model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 载入模型\n",
    "print('\\nLoading ONNX models...')\n",
    "\n",
    "# 1. 初始化 ONNX Runtime\n",
    "session_opts = onnxruntime.SessionOptions()\n",
    "session_opts.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session_opts.intra_op_num_threads = MAX_THREADS\n",
    "\n",
    "# 加载 Audio Encoder 和 Text Embedder\n",
    "# 注意：这里我们只需要这两个模型来生成 \"Fused Embedding\"\n",
    "ort_session_A = onnxruntime.InferenceSession(onnx_encoder, sess_options=session_opts, providers=['CPUExecutionProvider'])\n",
    "ort_session_B = onnxruntime.InferenceSession(onnx_embed, sess_options=session_opts, providers=['CPUExecutionProvider'])\n",
    "\n",
    "in_name_A = [x.name for x in ort_session_A.get_inputs()]\n",
    "out_name_A = [x.name for x in ort_session_A.get_outputs()]\n",
    "in_name_B = ort_session_B.get_inputs()[0].name\n",
    "out_name_B = [x.name for x in ort_session_B.get_outputs()]\n",
    "\n",
    "shape_value_in_A = ort_session_A._inputs_meta[0].shape[-1]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# 2. 加载 GGUF 模型\n",
    "print(f'\\nLoading GGUF model: {gguf_model_path}')\n",
    "llm = Llama(\n",
    "    model_path=gguf_model_path,\n",
    "    n_ctx=MAX_SEQ_LEN,\n",
    "    n_threads=MAX_THREADS,\n",
    "    embedding=True, # 必须开启，才能分配 embedding 内存\n",
    "    verbose=False\n",
    ")\n",
    "print('GGUF model loaded successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8c1f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test Input Audio: ./input.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 22:13:04,152 - INFO - 注入 Audio Embeddings Shape: (152, 1024)\n",
      "2026-01-18 22:13:04,161 - INFO - 正在注入融合 Embedding...\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 使用 GGUF 模型解码 (Low-Level API) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 22:13:05,275 - INFO - 开始生成文本 (最大 1024 tokens)...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，星期日欢迎收看一千零四期誓言消息，请静静介绍话题。去年十月十九日，九百六十七期节目说到委内瑞拉问题，我们回顾一下你当时的。"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 22:13:06,613 - INFO - 解码速度: 32.17 tokens/s (43 tokens in 1.34s)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 识别\n",
    "\n",
    "# 输入音频\n",
    "test_audio = r'./input.mp3'\n",
    "task_prompt = [\"将语音转写成中文：\"]  # 任务提示词 (在 Encoder 阶段融合)\n",
    "\n",
    "# 3. 预处理 Prompt\n",
    "# 将文本提示词转换为 Embedding\n",
    "init_all_outputs_B = []\n",
    "for t in task_prompt:\n",
    "    tokens = tokenizer(t, return_tensors='np')['input_ids'].astype(np.int32)\n",
    "    input_ids = onnxruntime.OrtValue.ortvalue_from_numpy(tokens, 'cpu', 0)\n",
    "    input_feed_B = {in_name_B: input_ids}\n",
    "    # 运行 Text Embedder\n",
    "    init_all_outputs_B.append(ort_session_B.run_with_ort_values(out_name_B, input_feed_B)[0])\n",
    "\n",
    "# 4. 处理音频并推理\n",
    "for prompt_embed, audio_file in zip(init_all_outputs_B, [test_audio]):\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Test Input Audio: {audio_file}\")\n",
    "    \n",
    "    # 加载和归一化音频\n",
    "    audio = np.array(AudioSegment.from_file(audio_file).set_channels(1).set_frame_rate(SAMPLE_RATE).get_array_of_samples(), dtype=np.int16)\n",
    "    if USE_NORMALIZER:\n",
    "        audio = normalizer(audio, 8192.0)\n",
    "        \n",
    "    audio_len = len(audio)\n",
    "    audio = audio.reshape(1, 1, -1)\n",
    "    \n",
    "    # 定义输入长度\n",
    "    if isinstance(shape_value_in_A, str):\n",
    "            INPUT_AUDIO_LENGTH = min(MAX_INPUT_AUDIO_LENGTH, audio_len)\n",
    "    else:\n",
    "            INPUT_AUDIO_LENGTH = shape_value_in_A\n",
    "            \n",
    "    stride_step = INPUT_AUDIO_LENGTH if SLIDING_WINDOW <= 0 else SLIDING_WINDOW\n",
    "    \n",
    "    # Padding 逻辑 (保持简单，针对短音频填充静音)\n",
    "    if audio_len < INPUT_AUDIO_LENGTH:\n",
    "            pad_len = INPUT_AUDIO_LENGTH - audio_len\n",
    "            pad_samples = np.zeros((1, 1, pad_len), dtype=audio.dtype) # 简单补零，或者补白噪声\n",
    "            audio = np.concatenate((audio, pad_samples), axis=-1)\n",
    "            \n",
    "    aligned_len = audio.shape[-1]\n",
    "    \n",
    "    asr_result = \"\"\n",
    "    slice_start = 0\n",
    "    slice_end = INPUT_AUDIO_LENGTH\n",
    "    rtf_time = time.time()\n",
    "    \n",
    "    # 循环处理音频切片\n",
    "    while slice_end <= aligned_len:\n",
    "        # 4.1 运行 ONNX Audio Encoder\n",
    "        input_feed_A = {}\n",
    "        input_feed_A[in_name_A[0]] = onnxruntime.OrtValue.ortvalue_from_numpy(audio[..., slice_start: slice_end], 'cpu', 0)\n",
    "        input_feed_A[in_name_A[1]] = prompt_embed # 注入 Task Prompt Embedding\n",
    "        \n",
    "        all_outputs_A = ort_session_A.run_with_ort_values(out_name_A, input_feed_A)\n",
    "        \n",
    "        # 获取融合 Embedding (Batch, Seq, Dim)\n",
    "        comprehensive_embedding = all_outputs_A[0].numpy() \n",
    "        \n",
    "        print(\"\\n=== 使用 GGUF 模型解码 (Low-Level API) ===\")\n",
    "        \n",
    "        try:\n",
    "            # 4.2 调用 Llama 模型进行解码\n",
    "            result_text = decode_with_pure_embeddings(\n",
    "                llm, \n",
    "                comprehensive_embedding,\n",
    "                max_new_tokens=MAX_SEQ_LEN\n",
    "            )\n",
    "            asr_result += result_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"解码发生错误: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # 4.3 更新切片窗口 (防止死循环!)\n",
    "        slice_start += stride_step\n",
    "        slice_end = slice_start + INPUT_AUDIO_LENGTH\n",
    "\n",
    "    # 打印最终统计\n",
    "    # print(f\"\\n\\nRTF: {((time.time() - rtf_time) / (audio_len / SAMPLE_RATE)):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funasr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
