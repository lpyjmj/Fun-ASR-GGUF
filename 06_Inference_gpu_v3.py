
import pickle
import numpy as np
import logging
import ctypes
import time
import os
import sys
from llama_cpp import (
    Llama,
    llama_batch_init,
    llama_batch_free,
    llama_decode,
    llama_get_logits,
    llama_kv_self_clear,
)

# =========================================================================
# æœ€ç»ˆé…ç½® (åŸºäºæµ‹è¯•ç»“æœè°ƒä¼˜)
# =========================================================================

# é»„é‡‘æ•°å€¼ï¼š8
# ç»æµ‹è¯•ï¼ŒVulkan åç«¯åœ¨æ­¤ç¡¬ä»¶ä¸Šä¸€æ¬¡æ³¨å…¥ > 8 ä¸ª Embedding ä¼šå¯¼è‡´ NaN
CHUNK_SIZE = 8 

# æ•°æ®ç±»å‹
# è™½ç„¶æµ‹è¯• False ä¹Ÿèƒ½è·‘ï¼Œä½†ä¸ºäº†é˜²æ­¢æ½œåœ¨çš„å†…å­˜é”™ä½ï¼Œæ ‡å‡†å·¥ç¨‹å®ç°å»ºè®® True
# è¿™é‡Œè®¾ä¸º True ä»¥ä¿è¯é•¿æœŸç¨³å®šæ€§ï¼Œä½ å¯ä»¥éšæ—¶æ”¹å› False
FORCE_FLOAT32 = True 

GGUF_MODEL_PATH = r'./model-gguf/qwen3-0.6b-asr.gguf'
PICKLE_PATH = r'./pickles/embedding_slice_0_160000.pkl' 

MAX_SEQ_LEN = 1024
STOP_TOKEN = [151643, 151645]

# =========================================================================
# ä¸»ç¨‹åº
# =========================================================================

def main():
    # 1. è‡ªåŠ¨å¯»æ‰¾æœ€æ–°çš„ pickle
    target_pickle = PICKLE_PATH
    if not os.path.exists(target_pickle):
        if os.path.exists("pickles"):
            files = [os.path.join("pickles", f) for f in os.listdir("pickles") if f.endswith(".pkl")]
            if files: target_pickle = max(files, key=os.path.getctime)
    
    print(f'\nğŸ“‚ Loading Pickle: {target_pickle}')
    with open(target_pickle, 'rb') as f:
        embeddings = pickle.load(f)
    
    # æ•°æ®é¢„å¤„ç†
    embeds = embeddings.squeeze()
    if len(embeds.shape) == 1: embeds = embeds.reshape(1, -1)
    
    if FORCE_FLOAT32:
        embeds = embeds.astype(np.float32)
        dtype_str = "float32 (Converted)"
    else:
        dtype_str = f"{embeds.dtype} (Raw)"

    n_tokens, n_dim = embeds.shape
    print(f"ğŸ“Š Data Shape: {embeds.shape} | Dtype: {dtype_str}")

    # 2. åŠ è½½æ¨¡å‹
    print(f'ğŸ¤– Loading Model: {GGUF_MODEL_PATH}')
    # æ³¨æ„ï¼šn_ubatch è®¾ä¸º 64 æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ‰‹åŠ¨æ§åˆ¶äº† batch å¤§å°ä¸º 8
    llm = Llama(
        model_path=GGUF_MODEL_PATH,
        n_ctx=MAX_SEQ_LEN + 2048,
        n_batch=2048,
        n_ubatch=64, 
        n_gpu_layers=-1,
        embedding=True,
        verbose=False 
    )
    print("âœ… Model Loaded (Vulkan Backend Active)")

    # 3. æ¨ç†è¿‡ç¨‹
    ctx = llm.ctx
    llama_kv_self_clear(ctx)
    llm.n_tokens = 0
    
    # åˆå§‹åŒ– Batch
    batch_embd = llama_batch_init(2048, n_dim, 1)
    batch_embd.token = ctypes.cast(None, ctypes.POINTER(ctypes.c_int32)) # æ ‡è®°ä¸º embedding
    
    batch_text = llama_batch_init(1, 0, 1)

    try:
        print(f"\nğŸš€ Start Injection (Chunk Size: {CHUNK_SIZE})...")
        inject_start = time.time()
        
        # --- æ ¸å¿ƒå¾ªç¯ï¼šåˆ†å—æ³¨å…¥ ---
        for i in range(0, n_tokens, CHUNK_SIZE):
            end = min(i + CHUNK_SIZE, n_tokens)
            current_len = end - i
            
            # å‡†å¤‡æ•°æ®åˆ‡ç‰‡
            chunk_data = embeds[i:end]
            if not chunk_data.flags['C_CONTIGUOUS']:
                chunk_data = np.ascontiguousarray(chunk_data)
                
            # è®¾ç½® Batch
            batch_embd.n_tokens = current_len
            for k in range(current_len):
                batch_embd.pos[k] = i + k
                batch_embd.n_seq_id[k] = 1
                batch_embd.seq_id[k][0] = 0
                # ä»…åœ¨æ•´ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ª token å¼€å¯ logits
                is_last = (i + k == n_tokens - 1)
                batch_embd.logits[k] = 1 if is_last else 0
            
            # å†…å­˜æ‹·è´
            ctypes.memmove(batch_embd.embd, chunk_data.ctypes.data, chunk_data.nbytes)
            
            # è§£ç 
            if llama_decode(ctx, batch_embd) != 0:
                print(f"âŒ Error during injection at index {i}")
                return
            
            llm_obj = llm # åˆ«å
            llm_obj.n_tokens += current_len
            
            # ç®€æ˜“è¿›åº¦æ¡
            if i % 32 == 0:
                sys.stdout.write('.')
                sys.stdout.flush()
                
        inject_time = time.time() - inject_start
        print(f"\nâœ… Injection Done. Time: {inject_time:.4f}s (Avg: {n_tokens/inject_time:.1f} t/s)")

        # --- æ–‡æœ¬ç”Ÿæˆ ---
        print("\nğŸ“ Generating Text:")
        print("-" * 40)
        
        vocab_size = llm.n_vocab()
        gen_start = time.time()
        gen_count = 0
        eos_token = llm.token_eos()
        
        full_text = ""
        
        for _ in range(MAX_SEQ_LEN):
            # è·å– Logits
            logits = np.ctypeslib.as_array(llama_get_logits(ctx), shape=(vocab_size,))
            token_id = int(np.argmax(logits))
            
            if token_id == eos_token or token_id in STOP_TOKEN:
                break
            
            # æ‰“å°å­—ç¬¦
            try:
                txt = llm.detokenize([token_id]).decode('utf-8', errors='ignore')
                print(txt, end="", flush=True)
                full_text += txt
                gen_count += 1
            except:
                pass
            
            # ä¸‹ä¸€æ­¥
            batch_text.n_tokens = 1
            batch_text.token[0] = token_id
            batch_text.pos[0] = llm.n_tokens
            batch_text.n_seq_id[0] = 1
            batch_text.seq_id[0][0] = 0
            batch_text.logits[0] = 1
            
            if llama_decode(ctx, batch_text) != 0:
                break
            llm.n_tokens += 1
            
        print("\n" + "-" * 40)
        gen_time = time.time() - gen_start
        print(f"âš¡ Generation Speed: {gen_count/gen_time:.2f} tokens/s")
        
    finally:
        llama_batch_free(batch_embd)
        llama_batch_free(batch_text)

if __name__ == "__main__":
    main()